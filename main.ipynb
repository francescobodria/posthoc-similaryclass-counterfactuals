{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb59c85c490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "random_seed = 42\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'fico'\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD Dataset\n",
    "from exp.data_loader import load_tabular_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_tabular_data(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Black Boxes\n",
    "\n",
    "# XGB\n",
    "from xgboost import XGBClassifier\n",
    "clf_xgb = XGBClassifier(n_estimators=60, reg_lambda=3, use_label_encoder=False, eval_metric='logloss')\n",
    "clf_xgb.fit(X_train, Y_train)\n",
    "clf_xgb.save_model(f'./blackboxes/{dataset_name}_xgboost')\n",
    "clf_xgb.load_model(f'./blackboxes/{dataset_name}_xgboost')\n",
    "y_train_pred = clf_xgb.predict(X_train)\n",
    "y_test_pred = clf_xgb.predict(X_test)\n",
    "print('XGB')\n",
    "print('train acc:',np.mean(np.round(y_train_pred)==Y_train))\n",
    "print('test acc:',np.mean(np.round(y_test_pred)==Y_test))\n",
    "\n",
    "#RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(random_state=random_seed)\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "pickle.dump(clf_rf,open(f'./blackboxes/{dataset_name}_rf.p','wb'))\n",
    "clf_rf = pickle.load(open(f'./blackboxes/{dataset_name}_rf.p','rb'))\n",
    "y_train_pred = clf_rf.predict(X_train)\n",
    "y_test_pred = clf_rf.predict(X_test)\n",
    "print('RF')\n",
    "print('train acc:',np.mean(np.round(y_train_pred)==Y_train))\n",
    "print('test acc:',np.mean(np.round(y_test_pred)==Y_test))\n",
    "\n",
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(gamma='auto', probability=True)\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "pickle.dump(clf_svc,open(f'./blackboxes/{dataset_name}_svc.p','wb'))\n",
    "clf_svc = pickle.load(open(f'./blackboxes/{dataset_name}_svc.p','rb'))\n",
    "y_train_pred = clf_svc.predict(X_train)\n",
    "y_test_pred = clf_svc.predict(X_test)\n",
    "print('SVC')\n",
    "print('train acc:',np.mean(np.round(y_train_pred)==Y_train))\n",
    "print('test acc:',np.mean(np.round(y_test_pred)==Y_test))\n",
    "\n",
    "#NN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "BATCH_SIZE = 1024\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(2048).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(BATCH_SIZE)\n",
    "clf_nn = keras.Sequential([\n",
    "    keras.layers.Dense(units=10, activation='relu'),\n",
    "    keras.layers.Dense(units=5, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "clf_nn.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = clf_nn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics)\n",
    "    plt.plot(epochs, val_metrics)\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid()\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "plot_metric(history, 'loss')\n",
    "clf_nn.save_weights(f'./blackboxes/{dataset_name}_tf_nn')\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf_nn.load_weights(f'./blackboxes/{dataset_name}_tf_nn')\n",
    "clf_nn.trainable = False\n",
    "print(accuracy_score(np.round(clf_nn.predict(X_train)),Y_train))\n",
    "print(accuracy_score(np.round(clf_nn.predict(X_test)),Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_box = 'xgb'\n",
    "\n",
    "if black_box=='xgb':\n",
    "    def predict(x, return_proba=False):\n",
    "        if return_proba:\n",
    "            return clf_xgb.predict_proba(x)[:,1].ravel()\n",
    "        else: return clf_xgb.predict(x).ravel().ravel()\n",
    "    y_test_pred = predict(X_test, return_proba=True)\n",
    "    y_train_pred = predict(X_train, return_proba=True)\n",
    "elif black_box=='rf':\n",
    "    def predict(x, return_proba=False):\n",
    "        if return_proba:\n",
    "            return clf_rf.predict_proba(x)[:,1].ravel()\n",
    "        else: return clf_rf.predict(x).ravel().ravel()\n",
    "    y_test_pred = predict(X_test, return_proba=True)\n",
    "    y_train_pred = predict(X_train, return_proba=True)\n",
    "elif black_box=='svc':\n",
    "    def predict(x, return_proba=False):\n",
    "        if return_proba:\n",
    "            return clf_svc.predict_proba(x)[:,1].ravel()\n",
    "        else: return clf_svc.predict(x).ravel().ravel()\n",
    "    y_test_pred = predict(X_test, return_proba=True)\n",
    "    y_train_pred = predict(X_train, return_proba=True)\n",
    "elif black_box=='nn':\n",
    "    def predict(x, return_proba=False):\n",
    "        if return_proba:\n",
    "            return clf_nn.predict(x).ravel()\n",
    "        else: return np.round(clf_nn.predict(x).ravel()).astype(int).ravel()\n",
    "    y_test_pred = predict(X_test, return_proba=True)\n",
    "    y_train_pred = predict(X_train, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Space\n",
    "X_train = np.hstack((X_train,y_train_pred.reshape(-1,1)))\n",
    "X_test = np.hstack((X_test,y_test_pred.reshape(-1,1)))\n",
    "\n",
    "latent_dim = 5\n",
    "batch_size = 1024\n",
    "sigma = 1\n",
    "max_epochs = 1000\n",
    "early_stopping = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "if dataset_name == 'adult':\n",
    "    idx_cat = [2,3,4,5,6]\n",
    "elif dataset_name == 'fico':\n",
    "    idx_cat = None\n",
    "elif dataset_name == 'german':\n",
    "    idx_cat = np.arange(3,71,1).tolist()\n",
    "elif dataset_name == 'compas':\n",
    "    idx_cat = list(range(13,33,1))\n",
    "    \n",
    "\n",
    "similarity_KLD = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "def compute_similarity_Z(Z, sigma):\n",
    "    D = 1 - F.cosine_similarity(X[:, None, :], X[None, :, :], dim=-1)\n",
    "    M = torch.exp((-D**2)/(2*sigma**2))\n",
    "    return M / (torch.ones([M.shape[0],M.shape[1]])*(torch.sum(M, axis = 0)-1)).transpose(0,1)\n",
    "\n",
    "def compute_similarity_X(X, sigma, idx_cat=None):\n",
    "    D_class = torch.cdist(X[:,-1].reshape(-1,1),X[:,-1].reshape(-1,1))\n",
    "    X = X[:, :-1]\n",
    "    if idx_cat:\n",
    "        X_cat = X[:, idx_cat]\n",
    "        X_cont = X[:, np.delete(range(X.shape[1]),idx_cat)]\n",
    "        h = X_cat.shape[1]\n",
    "        m = X.shape[1]\n",
    "        D_cont = 1 - F.cosine_similarity(X[:, None, :], X[None, :, :], dim=-1)\n",
    "        D_cat = torch.cdist(X_cat, X_cat, p=0)/h\n",
    "        D = h/m * D_cat + ((m-h)/m) * D_cont + D_class\n",
    "    else:\n",
    "        D_features = 1 - F.cosine_similarity(X[:, None, :], X[None, :, :], dim=-1) \n",
    "        D = D_features + D_class\n",
    "    M = torch.exp((-D**2)/(2*sigma**2))\n",
    "    return M / (torch.ones([M.shape[0],M.shape[1]])*(torch.sum(M, axis = 0)-1)).transpose(0,1)\n",
    "\n",
    "def loss_function(X, Z, idx_cat, sigma=1):\n",
    "    Sx = compute_similarity_X(X, sigma, idx_cat)\n",
    "    Sz = compute_similarity_Z(Z, 1)\n",
    "    loss = similarity_KLD(torch.log(Sx), Sz)\n",
    "    return loss\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dim=2):\n",
    "        super(LinearModel, self).__init__()\n",
    "\n",
    "        # encoding components\n",
    "        self.fc1 = nn.Linear(input_shape, latent_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return z\n",
    "\n",
    "# Create Model\n",
    "model = LinearModel(X_train.shape[1], latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "test_dataset = TensorDataset(torch.tensor(X_test).float())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "def check_and_clear(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    else:\n",
    "        os.system('rm -r ' + dir_name)\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "check_and_clear('./models/weights')\n",
    "\n",
    "model_params = list(model.parameters())\n",
    "optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "\n",
    "#validation parameters\n",
    "epoch = 1\n",
    "best = np.inf\n",
    "\n",
    "# progress bar\n",
    "pbar = tqdm(bar_format=\"{postfix[0]} {postfix[1][value]:03d} {postfix[2]} {postfix[3][value]:.5f} {postfix[4]} {postfix[5][value]:.5f} {postfix[6]} {postfix[7][value]:d}\",\n",
    "            postfix=[\"Epoch:\", {'value':0}, \"Train Sim Loss\", {'value':0}, \"Test Sim Loss\", {'value':0}, \"Early Stopping\", {\"value\":0}])\n",
    "\n",
    "# start training\n",
    "while epoch <= max_epochs:\n",
    "\n",
    "    # ------- TRAIN ------- #\n",
    "    # set model as training mode\n",
    "    model.train()\n",
    "    batch_loss = []\n",
    "\n",
    "    for batch, (X_batch,) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        Z_batch = model(X_batch)  #\n",
    "        loss  = loss_function(X_batch, Z_batch, sigma) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "    # save result\n",
    "    epoch_train_losses.append(np.mean(batch_loss))\n",
    "    pbar.postfix[3][\"value\"] = np.mean(batch_loss)\n",
    "\n",
    "    # -------- VALIDATION --------\n",
    "\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "    batch_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X_batch,) in enumerate(test_loader):\n",
    "            Z_batch = model(X_batch)\n",
    "            loss = loss_function(X_batch, Z_batch, sigma)\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "    # save information\n",
    "    epoch_test_losses.append(np.mean(batch_loss))\n",
    "    pbar.postfix[5][\"value\"] = np.mean(batch_loss)\n",
    "    pbar.postfix[1][\"value\"] = epoch\n",
    "\n",
    "    if epoch_test_losses[-1] < best:\n",
    "        wait = 0\n",
    "        best = epoch_test_losses[-1]\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), f'./models/weights/LinearTransparent_{dataset_name}.pt')\n",
    "    else:\n",
    "        wait += 1\n",
    "    pbar.postfix[7][\"value\"] = wait\n",
    "    if wait == early_stopping:\n",
    "        break    \n",
    "    epoch += 1\n",
    "    pbar.update()\n",
    "\n",
    "model.load_state_dict(torch.load(f'./models/weights/LinearTransparent_{dataset_name}.pt'))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Z_train = model(torch.tensor(X_train).float()).cpu().detach().numpy()\n",
    "    Z_test = model(torch.tensor(X_test).float()).cpu().detach().numpy()\n",
    "\n",
    "torch.save(model.state_dict(), f'./models/{dataset_name}_latent_{black_box}_{latent_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'./models/{dataset_name}_latent_{black_box}_{latent_dim}.pt'))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Z_train = model(torch.tensor(X_train).float()).cpu().detach().numpy()\n",
    "    Z_test = model(torch.tensor(X_test).float()).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z_train[:,0], Z_train[:,1], c=y_train_pred, cmap='coolwarm')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.fc1.weight.detach().numpy()\n",
    "b = model.fc1.bias.detach().numpy()\n",
    "y_contrib = model.fc1.weight.detach().numpy()[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cf(q, indexes):\n",
    "    q_pred = predict(q[:-1].reshape(1,-1),return_proba=True)\n",
    "    q_cf = q.copy()\n",
    "    q_cf_preds = []\n",
    "    q_cf_preds.append(float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)))\n",
    "    if q_pred > 0.5:\n",
    "        m = -0.1\n",
    "    else:\n",
    "        m = +0.1\n",
    "    while np.round(q_pred) == np.round(q_cf_preds[-1]):\n",
    "        v = np.array(model(torch.tensor(q_cf).float()).detach().numpy()+m*y_contrib)\n",
    "        c_l = [v[l] - np.sum(q_cf*w[l,:]) - b[l] for l in range(latent_dim)]\n",
    "        M = []\n",
    "        for l in range(latent_dim):\n",
    "            M.append([np.sum(w[k,indexes]*w[l,indexes]) for k in range(latent_dim)])\n",
    "        M = np.vstack(M)\n",
    "        lambda_k = np.linalg.solve(M, c_l)\n",
    "        delta_i = [np.sum(lambda_k*w[:,i]) for i in indexes]\n",
    "        q_cf[indexes] += delta_i\n",
    "        #q_cf = np.clip(q_cf,-1,1)\n",
    "        if float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)) in q_cf_preds:\n",
    "            return q_cf\n",
    "        q_cf_preds.append(float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)))\n",
    "        q_cf[-1] = q_cf_preds[-1]\n",
    "    return q_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "d_dist = []\n",
    "d_impl = []\n",
    "d_count = []\n",
    "d_adv = []\n",
    "num = []\n",
    "div_dist = []\n",
    "div_count = []\n",
    "\n",
    "for idx in tqdm(range(n)):\n",
    "    q = X_test[idx,:].copy()\n",
    "    q_pred = predict(q[:-1].reshape(1,-1),return_proba=False)\n",
    "    q_cfs = []\n",
    "    l_i = []\n",
    "    l_f = []\n",
    "\n",
    "    for indexes in list(combinations(list(range(X_train.shape[1]-1)),1)):    \n",
    "        q_cf = compute_cf(q, list(indexes))\n",
    "        q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)\n",
    "        if q_pred:\n",
    "            if q_cf_pred<0.5:\n",
    "                q_cfs.append(q_cf)\n",
    "        else:\n",
    "            if q_cf_pred>0.5:\n",
    "                q_cfs.append(q_cf) \n",
    "\n",
    "    for indexes in list(combinations(list(range(X_train.shape[1]-1)),2)):    \n",
    "        q_cf = compute_cf(q, list(indexes))\n",
    "        q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)\n",
    "        if q_pred:\n",
    "            if q_cf_pred<0.5:\n",
    "                q_cfs.append(q_cf)\n",
    "        else:\n",
    "            if q_cf_pred>0.5:\n",
    "                q_cfs.append(q_cf) \n",
    "        l_i.append([list(indexes),q_cf_pred])\n",
    "    r = np.argsort(np.stack(np.array(l_i,dtype=object)[:,1]).ravel())[-10:]\n",
    "    l_i = np.array(l_i,dtype=object)[r,0]\n",
    "\n",
    "    while len(l_i[0])<6:\n",
    "        for e in l_i:\n",
    "            for i in list(np.delete(range(X_train.shape[1]-1),e)):\n",
    "                q_cf = compute_cf(q, e+[i])\n",
    "                q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)\n",
    "                if q_pred:\n",
    "                    if q_cf_pred<0.5:\n",
    "                        q_cfs.append(q_cf)\n",
    "                else:\n",
    "                    if q_cf_pred>0.5:\n",
    "                        q_cfs.append(q_cf) \n",
    "                l_f.append([e+[i],q_cf_pred])\n",
    "        r = np.argsort(np.stack(np.array(l_f,dtype=object)[:,1]).ravel())[-10:]\n",
    "        l_f = np.array(l_f,dtype=object)[r,0]\n",
    "        l_i = l_f.copy()\n",
    "        l_f = []\n",
    "    \n",
    "    if len(q_cfs)<1:\n",
    "        continue\n",
    "    else:\n",
    "        q_cfs = np.vstack(q_cfs)\n",
    "        if black_box=='fico':\n",
    "            d_dist.append(np.min(cdist(q_cfs[:,:-1],q[:-1].reshape(1,-1))))\n",
    "            d_impl.append(np.min(cdist(q_cfs[:,:-1],X_train[:,:-1])))\n",
    "            d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))\n",
    "            r = np.argsort(cdist(q_cfs[:,:-1],X_train[:,:-1]),axis=1)[:,:10]\n",
    "            d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))\n",
    "            num.append(len(q_cfs))\n",
    "            div_dist.append(1/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1],q_cfs[:,:-1])))\n",
    "            div_count.append((X_train.shape[1]-1)/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))\n",
    "        elif black_box == 'adult':\n",
    "            d_dist.append(np.min(cdist(q_cfs[:,[2,3,4,5,6]],q[[2,3,4,5,6]].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,[0,1]],q[[0,1]].reshape(1,-1),metric='euclidean')))\n",
    "            d_impl.append(np.min(cdist(q_cfs[:,[2,3,4,5,6]],X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],X_train[:,[0,1]],metric='euclidean')))\n",
    "            d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))\n",
    "            r = np.argsort(cdist(q_cfs[:,[2,3,4,5,6]],X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],X_train[:,[0,1]],metric='euclidean'),axis=1)[:,:10]\n",
    "            d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))\n",
    "            num.append(len(q_cfs))\n",
    "            div_dist.append(np.mean(cdist(q_cfs[:,[2,3,4,5,6]],q_cfs[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],q_cfs[:,[0,1]],metric='euclidean')))\n",
    "            div_count.append(X_train.shape[1]-1/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))\n",
    "        elif black_box == 'compas':\n",
    "            d_dist.append(np.min(cdist(q_cfs[:,13:-1],q[13:-1].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,:13],q[:13].reshape(1,-1),metric='euclidean')))\n",
    "            d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))\n",
    "            d_impl.append(np.min(cdist(q_cfs[:,13:-1],X_train[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],X_train[:,:13],metric='euclidean')))\n",
    "            r = np.argsort(cdist(q_cfs[:,13:-1],X_train[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],X_train[:,:13],metric='euclidean'),axis=1)[:,:10]\n",
    "            d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))\n",
    "            num.append(len(q_cfs))\n",
    "            div_dist.append(np.mean(cdist(q_cfs[:,13:-1],q_cfs[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],q_cfs[:,:13],metric='euclidean')))\n",
    "            div_count.append(X_train.shape[1]-1/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))\n",
    "        elif black_box == 'german':\n",
    "            d_dist.append(np.min(cdist(q_cfs[:,3:-1],q[3:-1].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,:3],q[:3].reshape(1,-1),metric='euclidean')))\n",
    "            d_impl.append(np.min(cdist(q_cfs[:,3:-1],X_train[:,3:-1],metric='hamming') + cdist(q_cfs[:,:3],X_train[:,:3],metric='euclidean')))\n",
    "            d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))\n",
    "            num.append(len(q_cfs))\n",
    "            div_dist.append(np.mean(cdist(q_cfs[:,3:-1],q_cfs[:,3:-1],metric='hamming') + cdist(q_cfs[:,:3],q_cfs[:,:3],metric='euclidean')))\n",
    "            div_count.append(X_train.shape[1]-1/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('d_dist: \\t',    np.round(np.mean(d_dist),5),   np.round(np.std(d_dist),5))\n",
    "print('d_count: \\t',   np.round(np.mean(d_count),5),  np.round(np.std(d_count),5))\n",
    "print('implicity: \\t', np.round(np.mean(d_impl),5),   np.round(np.std(d_impl),5))\n",
    "print('d_adv: \\t\\t',   np.round(np.mean(d_adv),5),   np.round(np.std(d_adv),5))\n",
    "print('number: \\t',    np.round(np.mean(num),5),'\\t', np.round(np.std(num),5))\n",
    "print('div_dist: \\t',  np.round(np.mean(div_dist),5), np.round(np.std(div_dist),5))\n",
    "print('div_count: \\t', np.round(np.mean(div_count),5),np.round(np.std(div_count),5))\n",
    "\n",
    "print('success_rate: \\t', len(d_dist)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Growing Sphere\n",
    "\n",
    "from growingspheres import counterfactuals as cf\n",
    "\n",
    "d_dist_GS = []\n",
    "d_count_GS = []\n",
    "d_impl_GS = []\n",
    "d_adv_GS = []\n",
    "\n",
    "for idx in tqdm(range(n)):\n",
    "    q = X_test[idx,:-1].reshape(1,-1).copy()\n",
    "    pred = int(predict(q))\n",
    "    CF = cf.CounterfactualExplanation(q, predict, method='GS')\n",
    "    CF.fit(n_in_layer=2000, first_radius=0.1, dicrease_radius=10, sparse=True, verbose=False)\n",
    "    q_cf_GS = CF.enemy\n",
    "    if dataset_name == 'adult':\n",
    "        d_dist_GS.append(float(cdist(q_cf_GS[[2,3,4,5,6]].reshape(1,-1),q[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cf_GS[[0,1]].reshape(1,-1),q[:,[0,1]],metric='euclidean')))\n",
    "        d_count_GS.append(np.sum(q_cf_GS!=q))\n",
    "        d_impl_GS.append(np.min(cdist(q_cf_GS[[2,3,4,5,6]].reshape(1,-1),X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cf_GS[[0,1]].reshape(1,-1),X_train[:,[0,1]],metric='euclidean')))\n",
    "        r = np.argsort(cdist(q_cf_GS[[2,3,4,5,6]].reshape(1,-1),X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cf_GS[[0,1]].reshape(1,-1),X_train[:,[0,1]],metric='euclidean'),axis=1)[:,:10]\n",
    "        d_adv_GS.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==pred) for i in range(q_cf_GS.reshape(1,-1).shape[0])])))\n",
    "    elif dataset_name == 'fico':\n",
    "        d_dist_GS.append(euclidean(q_cf_GS,q))\n",
    "        d_count_GS.append(np.sum(q_cf_GS!=q))\n",
    "        d_impl_GS.append(np.min(cdist(q_cf_GS.reshape(1,-1),X_train[:,:-1])))\n",
    "        r = np.argsort(cdist(q_cf_GS.reshape(1,-1),X_train[:,:-1],metric='euclidean'),axis=1)[:,:10]\n",
    "        d_adv_GS.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==pred) for i in range(q_cf_GS.reshape(1,-1).shape[0])])))\n",
    "    elif dataset_name == 'german':\n",
    "        d_dist_GS.append(float(cdist(q_cf_GS[3:].reshape(1,-1),q[:,3:],metric='hamming') + cdist(q_cf_GS[:3].reshape(1,-1),q[:,:3],metric='euclidean')))\n",
    "        d_count_GS.append(np.sum(q_cf_GS!=q.ravel()))\n",
    "        d_impl_GS.append(np.min(cdist(q_cf_GS[3:].reshape(1,-1),X_train[:,3:],metric='hamming') + cdist(q_cf_GS[:3].reshape(1,-1),X_train[:,:3],metric='euclidean')))\n",
    "        r = np.argsort(cdist(q_cf_GS[3:].reshape(1,-1),X_train[:,3:],metric='hamming') + cdist(q_cf_GS[:3].reshape(1,-1),X_train[:,:3],metric='euclidean'),axis=1)[:,:10]\n",
    "        d_adv_GS.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==pred) for i in range(q_cf_GS.reshape(1,-1).shape[0])])))\n",
    "    elif dataset_name == 'compas':\n",
    "        d_dist_GS.append(float(cdist(q_cf_GS[13:].reshape(1,-1),q[:,13:],metric='hamming') + cdist(q_cf_GS[:13].reshape(1,-1),q[:,:13],metric='euclidean')))\n",
    "        d_count_GS.append(np.sum(q_cf_GS!=q))\n",
    "        d_impl_GS.append(np.min(cdist(q_cf_GS[13:].reshape(1,-1),X_train[:,13:-1],metric='hamming') + cdist(q_cf_GS[:13].reshape(1,-1),X_train[:,:13],metric='euclidean')))\n",
    "        r = np.argsort(cdist(q_cf_GS[13:].reshape(1,-1),X_train[:,13:-1],metric='hamming') + cdist(q_cf_GS[:13].reshape(1,-1),X_train[:,:13],metric='euclidean'),axis=1)[:,:10]\n",
    "        d_adv_GS.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==pred) for i in range(q_cf_GS.reshape(1,-1).shape[0])])))\n",
    "\n",
    "print('d_dist_GS: \\t',    np.round(np.mean(d_dist_GS),5),   np.round(np.std(d_dist_GS),5))\n",
    "print('d_count_GS: \\t',   np.round(np.mean(d_count_GS),5),  np.round(np.std(d_count_GS),5))\n",
    "print('d_impl_GS: \\t', np.round(np.mean(d_impl_GS),5),   np.round(np.std(d_impl_GS),5))\n",
    "print('d_adv_GS: \\t\\t',   np.round(np.mean(d_adv_GS),5),   np.round(np.std(d_adv_GS),5))\n",
    "print('success_rate: \\t', len(d_dist_GS)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watcher\n",
    "\n",
    "from scipy.spatial.distance import cdist, euclidean\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "\n",
    "d_dist_watch = []\n",
    "d_count_watch = []\n",
    "d_impl_watch = []\n",
    "d_adv_watch = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    # initial conditions\n",
    "    lamda = 0.1 \n",
    "    x0 = np.zeros([1,X_train.shape[1]-1]) # initial guess for cf\n",
    "    q = X_test[i:i+1,:-1].copy()\n",
    "    pred = predict(q,return_proba=False)\n",
    "\n",
    "    def dist_mad(cf, eg):\n",
    "        manhat = [cdist(eg.T, cf.reshape(1,-1).T ,metric='cityblock')[i][i] for i in range(len(eg.T))]\n",
    "        #mad = stats.median_absolute_deviation(X_train)\n",
    "        return sum(manhat)\n",
    "\n",
    "    def loss_function_mad(x_dash):\n",
    "        target = 1-pred\n",
    "        if target == 0:\n",
    "            L = lamda*(predict(x_dash.reshape(1,-1),return_proba=True)-1)**2 + dist_mad(x_dash.reshape(1,-1), q)\n",
    "        else:\n",
    "            L = lamda*(1-predict(x_dash.reshape(1,-1),return_proba=True)-1)**2 + dist_mad(x_dash.reshape(1,-1), q) \n",
    "        return L\n",
    "\n",
    "    res = minimize(loss_function_mad, x0, method='nelder-mead', options={'maxiter':100, 'xatol': 1e-6})\n",
    "    cf = res.x.reshape(1, -1)\n",
    "\n",
    "    i = 0\n",
    "    r = 1\n",
    "    while pred == predict(cf):\n",
    "        lamda += 0.1\n",
    "        x0 = cf \n",
    "        res = minimize(loss_function_mad, x0, method='nelder-mead', options={'maxiter':100, 'xatol': 1e-6})\n",
    "        cf = res.x.reshape(1, -1)\n",
    "        i += 1\n",
    "        if i == 100:\n",
    "            r = 0\n",
    "            break\n",
    "\n",
    "    if r == 1:\n",
    "        if dataset_name == 'adult':\n",
    "            d_dist_watch.append(float(cdist(cf[:,[2,3,4,5,6]],q[:,[2,3,4,5,6]],metric='hamming') + cdist(cf[:,[0,1]],q[:,[0,1]],metric='euclidean')))\n",
    "            d_count_watch.append(np.sum(cf!=q))\n",
    "            d_impl_watch.append(np.min(cdist(cf[:,[2,3,4,5,6]],X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(cf[:,[0,1]],X_train[:,[0,1]],metric='euclidean')))\n",
    "            r = np.argsort(cdist(cf[:,[2,3,4,5,6]],X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(cf[:,[0,1]],X_train[:,[0,1]],metric='euclidean'),axis=1)[:,:10]\n",
    "            d_adv_watch.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==pred) for i in range(cf.shape[0])])))\n",
    "        elif dataset_name == 'fico':\n",
    "            d_dist_watch.append(euclidean(cf,q))\n",
    "            d_count_watch.append(np.sum(cf!=q))\n",
    "            d_impl_watch.append(np.min(cdist(cf.reshape(1,-1),X_train[:,:-1])))\n",
    "            r = np.argsort(cdist(cf.reshape(1,-1),X_train[:,:-1],metric='euclidean'),axis=1)[:,:10]\n",
    "            d_adv_watch.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==pred) for i in range(q_cf_GS.reshape(1,-1).shape[0])])))\n",
    "        elif dataset_name == 'german':\n",
    "            d_dist_watch.append(float(cdist(cf[:,3:],q[:,3:],metric='hamming') + cdist(cf[:,:3],q[:,:3],metric='euclidean')))\n",
    "            d_count_watch.append(np.sum(cf!=q.ravel()))\n",
    "            d_impl_watch.append(np.min(cdist(cf[:,3:],X_train[:,3:],metric='hamming') + cdist(cf[:,:3],X_train[:,:3],metric='euclidean')))\n",
    "            r = np.argsort(cdist(cf[:,3:],X_train[:,3:],metric='hamming') + cdist(cf[:,:3],X_train[:,:3],metric='euclidean'),axis=1)[:,:10]\n",
    "            d_adv_watch.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==pred) for i in range(q_cf_GS.reshape(1,-1).shape[0])])))\n",
    "        elif dataset_name == 'compas':\n",
    "            d_dist_watch.append(float(cdist(cf[:,13:],q[:,13:],metric='hamming') + cdist(cf[:,:13],q[:,:13],metric='euclidean')))\n",
    "            d_count_watch.append(np.sum(q_cf_GS!=q))\n",
    "            d_impl_watch.append(np.min(cdist(cf[:,13:],X_train[:,13:-1],metric='hamming') + cdist(cf[:,:13],X_train[:,:13],metric='euclidean')))\n",
    "            r = np.argsort(cdist(cf[:,13:],X_train[:,13:-1],metric='hamming') + cdist(cf[:,:13],X_train[:,:13],metric='euclidean'),axis=1)[:,:10]\n",
    "            d_adv_watch.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==pred) for i in range(q_cf_GS.reshape(1,-1).shape[0])])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe59c975e204ea16b5438cc29498c66156cc1f3ecfcff94114e4056e991a222"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('latent_space')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import pickle
pd.set_option('display.max_columns', None)
import time
import warnings
warnings.filterwarnings("ignore")

import torch
from sklearn import datasets, svm
from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors, LocalOutlierFactor
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score
from scipy.spatial.distance import cdist, pdist
from numpy.random import default_rng
from collections import Counter
from sklearn.cluster import KMeans
from numpy.linalg import norm

import boto3
import smtplib, ssl

AWS_REGION = "eu-west-1"
S3_BUCKET_NAME = "transparentfeaturereduction"

s3_client = boto3.client("s3", region_name=AWS_REGION)

def upload_files(file_name, bucket, object_name=None, args=None):
    if object_name is None:
        object_name = file_name
    s3_client.upload_file(file_name, bucket, object_name, ExtraArgs=args)
    #print(f"'{file_name}' has been uploaded to '{S3_BUCKET_NAME}'")

class Mail:
    def __init__(self):
        self.port = 465
        self.smtp_server_domain_name = "smtp.gmail.com"
        self.sender_mail = "francesco.bodria@sns.it"
        self.password = "ysjrwdfhehikunui"
    def send(self, emails, subject, content):
        ssl_context = ssl.create_default_context()
        service = smtplib.SMTP_SSL(self.smtp_server_domain_name, self.port, context=ssl_context)
        service.login(self.sender_mail, self.password)
        for email in emails:
            result = service.sendmail(self.sender_mail, email, f"Subject: {subject}\n{content}")
        service.quit()

def knn_clf(nbr_vec, y):
    '''
    Helper function to generate knn classification result.
    '''
    y_vec = y[nbr_vec]
    c = Counter(y_vec)
    return c.most_common(1)[0][0]

def knn_eval_series(X, y, n_neighbors_list=[1, 2, 3, 4, 5, 10, 15, 20], n_jobs=-1):
    '''
    This is a function that is used to evaluate the lower dimension embedding.
    An accuracy is calculated by an k-nearest neighbor classifier.
    A series of accuracy will be calculated for the given n_neighbors.
    Input:
        X: A numpy array with the shape [N, k]. The lower dimension embedding
           of some dataset. Expected to have some clusters.
        y: A numpy array with the shape [N, 1]. The labels of the original
           dataset.
        n_neighbors_list: A list of int.
        kwargs: Any keyword argument that is send into the knn clf.
    Output:
        accs: The avg accuracy generated by the clf, using leave one out cross val.
    '''
    avg_accs = []
    max_acc = X.shape[0]
    # Train once, reuse multiple times
    nbrs = NearestNeighbors(n_neighbors=n_neighbors_list[-1]+1, n_jobs=n_jobs).fit(X)
    distances, indices = nbrs.kneighbors(X)
    indices = indices [:, 1:]
    distances = distances[:, 1:]
    for n_neighbors in n_neighbors_list:
        sum_acc = 0
        for i in range(X.shape[0]):
            indices_temp = indices[:, :n_neighbors]
            result = knn_clf(indices_temp[i], y)
            if result == y[i]:
                sum_acc += 1
        avg_acc = sum_acc / max_acc
        avg_accs.append(avg_acc)
    return 1-np.array(avg_accs)

def random_triplet_eval(X, X_new, y):
    '''
    This is a function that is used to evaluate the lower dimension embedding.
    An triplet satisfaction score is calculated by evaluating how many randomly
    selected triplets have been violated. Each point will generate 5 triplets.
    Input:
        X: A numpy array with the shape [N, p]. The higher dimension embedding
           of some dataset. Expected to have some clusters.
        X_new: A numpy array with the shape [N, k]. The lower dimension embedding
               of some dataset. Expected to have some clusters as well.
        y: A numpy array with the shape [N, 1]. The labels of the original
           dataset. Used to identify clusters
    Output:
        acc: The score generated by the algorithm.
    '''    

    # Sampling Triplets
    # Five triplet per point
    anchors = np.arange(X.shape[0])
    rng = default_rng()
    triplets = rng.choice(anchors, (X.shape[0], 5, 2))
    triplet_labels = np.zeros((X.shape[0], 5))
    anchors = anchors.reshape((-1, 1, 1))
    
    # Calculate the distances and generate labels
    b = np.broadcast(anchors, triplets)
    distances = np.empty(b.shape)
    distances.flat = [np.linalg.norm(X[u] - X[v]) for (u,v) in b]
    labels = distances[:, :, 0] < distances[: , :, 1]

    # Calculate distances for LD
    b = np.broadcast(anchors, triplets)
    distances_l = np.empty(b.shape)
    distances_l.flat = [np.linalg.norm(X_new[u] - X_new[v]) for (u,v) in b]
    pred_vals = distances_l[:, :, 0] < distances_l[:, :, 1]
    correct = np.sum(pred_vals == labels)
    acc = correct/X.shape[0]/5
    return acc

def lof_eval(X, Z, n_jobs=-1):
    clf = LocalOutlierFactor(n_jobs=n_jobs)
    clf.fit(X)
    outlier_factor_input_space = clf.negative_outlier_factor_
    clf = LocalOutlierFactor(n_jobs=n_jobs)
    clf.fit(Z)
    outlier_factor_latent_space = clf.negative_outlier_factor_
    lof_score = np.mean((outlier_factor_input_space-outlier_factor_latent_space)**2)
    return lof_score
 
def isf_eval(X, Z, n_jobs=-1):
    clf = IsolationForest(n_jobs=n_jobs)
    clf.fit(X)
    outlier_factor_input_space = clf.score_samples(X)
    clf = IsolationForest(n_jobs=n_jobs)
    clf.fit(Z)
    outlier_factor_latent_space = clf.score_samples(Z)
    isf_score = np.mean((outlier_factor_input_space-outlier_factor_latent_space)**2)
    return isf_score

def sse_eval(Z):
    kmeans = KMeans(n_clusters=2).fit(Z)
    sse_score = kmeans.inertia_
    return sse_score

def spars_eval(model, Z):
    y_contrib = model.fc1.weight.detach().numpy()[:,-1]
    thetas = np.arccos(np.round(np.dot(Z/norm(Z,axis=1).reshape(-1,1),y_contrib/norm(y_contrib)),5))
    spars_score = np.std(np.linalg.norm(Z)*np.cos(thetas))
    return spars_score

def compute_metrics(model, X, Z, Y, n_jobs=-1):    
    knn_score = np.mean(knn_eval_series(Z, Y, n_jobs=n_jobs))
    triplet_score = random_triplet_eval(X, Z, Y)
    lof_score = lof_eval(X, Z, n_jobs=n_jobs)
    isf_score = isf_eval(X, Z, n_jobs=n_jobs)
    sse_score = sse_eval(Z)
    spars_score = spars_eval(model, Z)
    return {'KNN':knn_score,
            'Triplet':triplet_score,
            'LOF':lof_score,
            'IsF':isf_score,
            'sse':sse_score,
            'spars':spars_score}

d = {}

for dataset_name in ['german', 'compas']:
    print(dataset_name)
    d[dataset_name]={}
    for bb_name in ['nn']:
        print(bb_name)
        d[dataset_name]={bb_name:{}}

        from exp.data_loader import load_tabular_data
        from exp.bb_loader import load_bb
        X_train, X_test, y_train, y_test = load_tabular_data(dataset_name)
        y_train_pred, y_test_pred, clf = load_bb(X_train, X_test, dataset_name, bb_name)
        X_train = np.hstack((X_train,y_train_pred.reshape(-1,1)))
        y_train = y_train.values
        X_test = np.hstack((X_test,y_test_pred.reshape(-1,1)))
        y_test = y_test.values

        import torch
        import torch.nn as nn
        import torch.nn.functional as F

        class LinearModel(nn.Module):
            def __init__(self, input_shape, latent_dim=2):
                super(LinearModel, self).__init__()
                # encoding components
                self.fc1 = nn.Linear(input_shape, latent_dim)
            def encode(self, x):
                x = self.fc1(x)
                return x
            def forward(self, x):
                z = self.encode(x)
                return z

        if dataset_name == 'adult':
            latent_dims = [2,3,4,5,6,7,8]
        elif dataset_name == 'fico':
            latent_dims = [2,3,4,6,9,12,15,18,21,24]
        elif dataset_name == 'german':
            latent_dims = [2,3,5,10,20,30,40,50,60,71]
        elif dataset_name == 'compas':
            latent_dims = [2,3,4,5,7,10,15,20,25,33]

        for latent_dim in latent_dims:
            model = LinearModel(X_train.shape[1], latent_dim=latent_dim)
            
            model.load_state_dict(torch.load(f'./models/{dataset_name}_latent_{bb_name}_{latent_dim}.pt'))
            with torch.no_grad():
                model.eval()
                Z_train = model(torch.tensor(X_train).float()).cpu().detach().numpy()
                Z_test = model(torch.tensor(X_test).float()).cpu().detach().numpy()

            d[dataset_name][bb_name][str(latent_dim)] = compute_metrics(model, X_train, Z_train, y_train)

        fig, ax = plt.subplots(3,2,figsize=(30,15))
        ax.ravel()[0].plot(latent_dims,[r['KNN'] for r in d[dataset_name][bb_name].values()], '-o')
        ax.ravel()[0].grid()
        ax.ravel()[0].set_xticks(latent_dims)
        ax.ravel()[0].set_title('KNN')
        ax.ravel()[1].plot(latent_dims,[r['Triplet'] for r in d[dataset_name][bb_name].values()], '-o')
        ax.ravel()[1].grid()
        ax.ravel()[1].set_xticks(latent_dims)
        ax.ravel()[1].set_title('Triplet')
        ax.ravel()[2].plot(latent_dims,[r['LOF'] for r in d[dataset_name][bb_name].values()], '-o')
        ax.ravel()[2].grid()
        ax.ravel()[2].set_xticks(latent_dims)
        ax.ravel()[2].set_title('LOF')
        ax.ravel()[3].plot(latent_dims,[r['IsF'] for r in d[dataset_name][bb_name].values()], '-o')
        ax.ravel()[3].grid()
        ax.ravel()[3].set_xticks(latent_dims)
        ax.ravel()[3].set_title('IsF')
        ax.ravel()[4].plot(latent_dims,[r['sse'] for r in d[dataset_name][bb_name].values()], '-o')
        ax.ravel()[4].grid()
        ax.ravel()[4].set_xticks(latent_dims)
        ax.ravel()[4].set_title('sse')
        ax.ravel()[4].set_xlabel('latent_dim')
        ax.ravel()[5].plot(latent_dims,[r['spars'] for r in d[dataset_name][bb_name].values()], '-o')
        ax.ravel()[5].grid()
        ax.ravel()[5].set_xticks(latent_dims)
        ax.ravel()[5].set_title('spars')
        ax.ravel()[5].set_xlabel('latent_dim')
        #fig.savefig(f'./plots/{dataset_name}_{bb_name}_space_metrics.jpeg', bbox_inches='tight')
        #upload_files(f'/home/ubuntu/posthoc-similaryclass-counterfactuals/plots/{dataset_name}_{bb_name}_space_metrics.jpeg', S3_BUCKET_NAME)
        plt.close()

        if bb_name in ['xgb', 'rf', 'svc']:
            def predict(x, return_proba=False):
                if return_proba:
                    return clf.predict_proba(x)[:,1].ravel()
                else: return clf.predict(x).ravel().ravel()
        else:
            def predict(x, return_proba=False):
                if return_proba:
                    return clf.predict(x).ravel()
                else: return np.round(clf.predict(x).ravel()).astype(int).ravel()
        
        def compute_cf(q, indexes, max_iter=1000):
            q_pred = predict(q[:-1].reshape(1,-1),return_proba=True)
            q_cf = q.copy()
            q_cf_preds = []
            q_cf_preds.append(float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)))
            if q_pred > 0.5:
                m = -0.1
            else:
                m = +0.1
            i = 0
            while np.round(q_pred) == np.round(q_cf_preds[-1]):
                if i > max_iter:
                    break
                v = np.array(model(torch.tensor(q_cf).float()).detach().numpy()+m*y_contrib)
                c_l = [v[l] - np.sum(q_cf*w[l,:]) - b[l] for l in range(latent_dim)]
                M = []
                for l in range(latent_dim):
                    M.append([np.sum(w[k,indexes]*w[l,indexes]) for k in range(latent_dim)])
                M = np.vstack(M)
                lambda_k = np.linalg.solve(M, c_l)
                delta_i = [np.sum(lambda_k*w[:,i]) for i in indexes]
                q_cf[indexes] += delta_i
                if float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)) in q_cf_preds:
                    return q_cf
                q_cf_preds.append(float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)))
                q_cf[-1] = q_cf_preds[-1]
                i += 1
            return q_cf

        for latent_dim in latent_dims:
            model = LinearModel(X_train.shape[1], latent_dim=latent_dim)
            model.load_state_dict(torch.load(f'./models/{dataset_name}_latent_{bb_name}_{latent_dim}.pt'))
            with torch.no_grad():
                model.eval()
                Z_train = model(torch.tensor(X_train).float()).cpu().detach().numpy()
                Z_test = model(torch.tensor(X_test).float()).cpu().detach().numpy()
            
            w = model.fc1.weight.detach().numpy()
            b = model.fc1.bias.detach().numpy()
            y_contrib = model.fc1.weight.detach().numpy()[:,-1]

            from itertools import combinations

            d_dist = []
            d_impl = []
            d_count = []
            d_adv = []
            num = [] 
            div_dist = []
            div_count = []

            if dataset_name == 'fico':
                idx_len = 23
            elif dataset_name == 'adult':
                idx_len = 7
            elif dataset_name == 'compas':
                idx_len = 33
            elif dataset_name == 'german':
                idx_len = 71

            q_cfs_list = []

            for idx in tqdm(range(10)):
                q = X_test[idx,:].copy()
                q_pred = predict(q[:-1].reshape(1,-1),return_proba=False)
                q_cfs = []
                l_i = []
                l_f = []

                for indexes in list(combinations(list(range(idx_len)),1)):    
                    q_cf = compute_cf(q, list(indexes))
                    q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)
                    if q_pred:
                        if q_cf_pred<0.5:
                            q_cfs.append(q_cf)
                    else:
                        if q_cf_pred>0.5:
                            q_cfs.append(q_cf) 
                
                for indexes in list(combinations(list(range(idx_len)),2)):    
                    q_cf = compute_cf(q, list(indexes))
                    q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)
                    if q_pred:
                        if q_cf_pred<0.5:
                            q_cfs.append(q_cf)
                    else:
                        if q_cf_pred>0.5:
                            q_cfs.append(q_cf) 
                    l_i.append([list(indexes),q_cf_pred])
                r = np.argsort(np.stack(np.array(l_i,dtype=object)[:,1]).ravel())[-10:]
                l_i = np.array(l_i,dtype=object)[r,0]
                
                while len(l_i[0])<6:
                    for e in l_i:
                        for i in list(np.delete(range(idx_len),e)):
                            q_cf = compute_cf(q, e+[i])
                            q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)
                            if q_pred:
                                if q_cf_pred<0.5:
                                    q_cfs.append(q_cf)
                            else:
                                if q_cf_pred>0.5:
                                    q_cfs.append(q_cf) 
                            l_f.append([e+[i],q_cf_pred])
                    r = np.argsort(np.stack(np.array(l_f,dtype=object)[:,1]).ravel())[-10:]
                    l_f = np.array(l_f,dtype=object)[r,0]
                    l_i = l_f.copy()
                    l_f = []
                
                if len(q_cfs)<1:
                    continue
                else:
                    q_cfs = np.vstack(q_cfs)
                    q_cfs_list.append(q_cfs)
                    if dataset_name == 'fico':
                        d_dist.append(np.min(cdist(q_cfs[:,:-1],q[:-1].reshape(1,-1))))
                        d_impl.append(np.min(cdist(q_cfs[:,:-1],X_train[:,:-1])))
                        d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))
                        r = np.argsort(cdist(q_cfs[:,:-1],X_train[:,:-1]),axis=1)[:,:10]
                        d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))
                        num.append(len(q_cfs))
                        div_dist.append(np.mean(cdist(q_cfs[:,:-1],q_cfs[:,:-1])))
                        div_count.append(np.mean(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))
                    elif dataset_name == 'compas':
                        d_dist.append(np.min(cdist(q_cfs[:,13:-1],q[13:-1].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,:13],q[:13].reshape(1,-1),metric='euclidean')))
                        d_impl.append(np.min(cdist(q_cfs[:,13:-1],X_train[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],X_train[:,:13],metric='euclidean')))
                        d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))
                        r = np.argsort(cdist(q_cfs[:,13:-1],X_train[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],X_train[:,:13],metric='euclidean'),axis=1)[:,:10]
                        d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))
                        num.append(len(q_cfs))
                        div_dist.append(np.mean(cdist(q_cfs[:,13:-1],q_cfs[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],q_cfs[:,:13],metric='euclidean')))
                        div_count.append(idx_len/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))
                    elif dataset_name == 'adult':
                        d_dist.append(np.min(cdist(q_cfs[:,[2,3,4,5,6]],q[[2,3,4,5,6]].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,[0,1]],q[[0,1]].reshape(1,-1),metric='euclidean')))
                        d_impl.append(np.min(cdist(q_cfs[:,[2,3,4,5,6]],X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],X_train[:,[0,1]],metric='euclidean')))
                        d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))
                        r = np.argsort(cdist(q_cfs[:,[2,3,4,5,6]],X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],X_train[:,[0,1]],metric='euclidean'),axis=1)[:,:10]
                        d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))
                        num.append(len(q_cfs))
                        div_dist.append(np.mean(cdist(q_cfs[:,[2,3,4,5,6]],q_cfs[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],q_cfs[:,[0,1]],metric='euclidean')))
                        div_count.append(idx_len/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))
                    elif dataset_name == 'german':
                        d_dist.append(np.min(cdist(q_cfs[:,3:-1],q[3:-1].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,:3],q[:3].reshape(1,-1),metric='euclidean')))
                        d_impl.append(np.min(cdist(q_cfs[:,3:-1],X_train[:,3:-1],metric='hamming') + cdist(q_cfs[:,:3],X_train[:,:3],metric='euclidean')))
                        d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))
                        r = np.argsort(cdist(q_cfs[:,3:-1],X_train[:,3:-1],metric='hamming') + cdist(q_cfs[:,:3],X_train[:,:3],metric='euclidean'),axis=1)[:,:10]
                        d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))
                        num.append(len(q_cfs))
                        div_dist.append(np.mean(cdist(q_cfs[:,3:-1],q_cfs[:,3:-1],metric='hamming') + cdist(q_cfs[:,:3],q_cfs[:,:3],metric='euclidean')))
                        div_count.append(idx_len/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))
                
            d[dataset_name][bb_name][str(latent_dim)]['cfs'] = q_cfs_list
            d[dataset_name][bb_name][str(latent_dim)]['d_dist'] = np.mean(np.array(d_dist))
            d[dataset_name][bb_name][str(latent_dim)]['d_count'] = np.mean(np.array(d_count))
            d[dataset_name][bb_name][str(latent_dim)]['d_impl'] = np.mean(np.array(d_impl)) 
            d[dataset_name][bb_name][str(latent_dim)]['d_advs'] = np.mean(np.array(d_adv)) 
            d[dataset_name][bb_name][str(latent_dim)]['div_count'] = np.mean(np.array(div_dist)) 
            d[dataset_name][bb_name][str(latent_dim)]['div_dist'] = np.mean(np.array(div_count)) 

        fig, ax = plt.subplots(6,1,figsize=(20,15))
        ax[0].plot(latent_dims,[r['d_dist'] for r in d[dataset_name][bb_name].values()],'-o')
        ax[0].set_ylabel('d_dist')
        ax[0].set_xticks(latent_dims)
        ax[0].grid(True)
        ax[1].plot(latent_dims,[r['d_count'] for r in d[dataset_name][bb_name].values()],'-o')
        ax[1].set_ylabel('d_count')
        ax[1].grid(True)
        ax[1].set_xticks(latent_dims)
        ax[2].plot(latent_dims,[r['d_impl'] for r in d[dataset_name][bb_name].values()],'-o')
        ax[2].set_ylabel('d_impl')
        ax[2].grid(True)
        ax[2].set_xticks(latent_dims)
        ax[3].plot(latent_dims,[r['d_advs'] for r in d[dataset_name][bb_name].values()],'-o')
        ax[3].set_xlabel('latent_dim')
        ax[3].set_ylabel('d_advs')
        ax[3].grid(True)
        ax[3].set_xticks(latent_dims)
        ax[4].plot(latent_dims,[r['div_dist'] for r in d[dataset_name][bb_name].values()],'-o')
        ax[4].set_xlabel('latent_dim')
        ax[4].set_ylabel('div_dist')
        ax[4].grid(True)
        ax[4].set_xticks(latent_dims)
        ax[5].plot(latent_dims,[r['div_count'] for r in d[dataset_name][bb_name].values()],'-o')
        ax[5].set_xlabel('latent_dim')
        ax[5].set_ylabel('div_count')
        ax[5].grid(True)
        ax[5].set_xticks(latent_dims)
        fig.savefig(f'./plots/{dataset_name}_{bb_name}_cf_metrics.jpeg', bbox_inches='tight')
        #upload_files(f'/home/ubuntu/posthoc-similaryclass-counterfactuals/plots/{dataset_name}_{bb_name}_cf_metrics.jpeg', S3_BUCKET_NAME)
        plt.close()

        pickle.dump(d, open(f'./{dataset_name}_nn_results.p','wb'))
    
    #mails = ['francesco.bodria@sns.it']
    #subject = 'risultati esperimento'
    #content = f'{dataset_name} completato'
    #mail = Mail()
    #mail.send(mails, subject, content)



